---
title: "Donation Prediction for Direct Marketing"
author: "STAT 420, Summer 2021"
date: ''
output:
  html_document: 
    theme: readable
    toc: yes
  pdf_document: default
urlcolor: cyan
bibliography: bibliography.bib
---
***

```{r setup, echo = FALSE, message = FALSE, warning = FALSE}
options(scipen = 1, digits = 4, width = 80, fig.align = "center")
library(knitr)
opts_chunk$set(cache = TRUE, autodep = TRUE)
```

### Introduction

According to the @nccs, as of 2016, over 1.54 million nonprofit organizations were registered in the United States with the IRS. As is the case for most nonprofit organizations, there is reliance on charitable contributions to ensure their ability to continue to deliver their services and value to their respective communities. To help maximize donation profits for such charities, statistical models can be leveraged to provide a better understanding of their ideal donors. Predictive insights, like being able to predict donation amounts before sending a request via direct mail, could improve donor targeting practices, potentially yielding greater profits while minimizing the efforts of valuable marketing resources. With this project, we will be attempting to build a statistical model capable of doing just this. Specifically, we aim to use donor information, variables in the data set, from a national veterans' agency to predict the amount (in dollars) of each donor's current contribution to said national veteran's agency. 

_Links to the above data sets can be found below in the [References] section._

#### Data Set Background Information

The data file for this project was obtained from the @statlib.  The file contains data sampled from a much larger data set from the 1998 KDD Cup, an annual data science competition put on by the Association for Computing Machinery's @kdd_data. For the competition, a data set containing 95412 observations and 481 variables was provided. Likely to aid in ease of analysis, the DSAL generated a sample of the same data set, which we will use for this analysis, that contains 7600 observations with 19 variables for each observation. As was the case during the 1998 KDD Cup, each de-identified observation in the data set contains information for someone that previously donated to a Congressionally-chartered US veterans' service organization or one of its affiliates. At the time, this national veterans' agency represented the special interests of paralyzed veterans and their families and obtained most of its funding from charitable donations.

### Methods

##### Data File Description

- File Information
  - Name: `nvo.txt`
  - Type: standard text document
  - Encoding: UTF-8
  - Delimitation Method: Tab
- Dataset Characteristics
  - Each record contains information about a donor that has made charitable contributions to a national veteran's agency
  - 7600 records
  - 19 variables
    - 17 numeric variables _(6 of these will be ignored for this analysis)_
    - 2 categorical variables
- Variables of Interest
  - Response Variable
    - `Current.Gift`: the amount of the donor's current donation (in dollars).
  - Explanatory Variables
    - `Age`: the age of the donor
    - `Own.Home.`: a categorical variable regarding whether or not the donor owns a home
    - `Num.Children`: the number of children, if any, that the donor has
    - `Total.Wealth`: a wealth rating from 0-9 (with 9 being the wealthiest) that is based on median family income and population statistics
    - `Sex`: the gender of the donor
    - `Number.of.Gifts`: the number of donations made to this organization
    - `Other.Gifts`: number of times the donor has responded to a mail order donation offer for other affiliated organizations    
    - `Time.Between.Gifts`: the avg number of months between donations
    - `Largest.Gift`: the largest donation made (in dollars)
    - `Smallest.Gift`: the smallest donation made (in dollars)
    - `Previous.Gift`: the amount of the donor's previous donation (in dollars)
    - `Average.Gift` (not used): the avg donation amount (in dollars) from a donor 


##### Data Preparation

The following libraries were first loaded as they were used throughout the analysis.
```{r, message = FALSE, warning = FALSE}

# A dependency for bptest()
library(lmtest)

# Used to obtain variance inflation factors
library(faraway)

# Used for a pretty-print of our data
library(tibble)
```

Data was loaded from `nvo.txt`.
```{r}
nvo_data = read.table("nvo.txt", sep = "\t",header = TRUE, fileEncoding = "UTF-8")
nvo_data = as_tibble(nvo_data)
nvo_data
```

Once the data was loaded, the following categorical variables were first coerced as factors:

- `Sex`
- `Own.Home.`
- `Total.Wealth`

```{r}
nvo_data$Sex = as.factor(nvo_data$Sex)
head(nvo_data$Sex)
```

```{r}
nvo_data$Own.Home. = as.factor(nvo_data$Own.Home.)
head(nvo_data$Own.Home.)
```

```{r}
nvo_data$Total.Wealth = as.factor(nvo_data$Total.Wealth)
head(nvo_data$Total.Wealth)
```

Then unnecessary variables were removed:

- This data set included `sqrt()` transformed values of all `.Gift` variables that were not needed.
- The `Average.Gift` variable was excluded to avoid collinearity issues. `Average.Gift` is technically a function of the desired response (`Current.Gift`) and other valuable `.Gift` predictors (`Largest.Gift`, `Smallest.Gift`, and `Previous.Gift`) in that these other values are required to calculate the average gift amount for the donor.
- `Income` was removed for collinearity issues as well since (likely) the `Total.Wealth` variable is a function of `Income` and other factors.

```{r}
nvo_cur_data = subset(nvo_data, select = -c(
	Average.Gift
	,Income
	,Sqrt.Smallest.Gift
	,Sqrt.Current.Gift
	,Sqrt.Largest.Gift
	,Sqrt.Previous.Gift
	,Sqrt.Average.Gift)
	)
nvo_cur_data
```

Finally, observations that did not make logical sense were removed for this analysis.

Because we were interested in predicting the size of a donor's current gift and not whether or not a donor sent a donation, records where the `Current.Gift` = \$0 were removed. This removed 2,808 of the 7,600 observations.
```{r}
length(nvo_cur_data[nvo_cur_data$Current.Gift == 0, ]$Current.Gift)
nvo_cur_data = nvo_cur_data[nvo_cur_data$Current.Gift > 0, ]
```

A few additional records with a `Smallest.Gift` value of \$0 and/or a `Largest.Gift` value of \$0 were removed because, logically, these didn't make complete sense based on our assumptions about the data set. Namely, in order to be included in the data set, a donor had to have donated to the organization previously, which would require _some_ donation > 0, which would require both their smallest and largest donation to be greater than \$0.  Even if they donated \$0.01 once, both `Smallest.Gift` and `Largest.Gift` would be greater than 0 with a value of \$0.01.

31 observations with `Smallest.Gift` = 0 were removed:
```{r}
length(nvo_cur_data[nvo_cur_data$Smallest.Gift  == 0, ]$Smallest.Gift)
nvo_cur_data = nvo_cur_data[nvo_cur_data$Smallest.Gift > 0, ]
```
After the steps above, there weren't any remaining observations with `Largest.Gift` = \$0:
```{r}
length(nvo_cur_data[nvo_cur_data$Largest.Gift  == 0, ]$Largest.Gift)
```
It is also worth noting that observations with a `Previous.Gift` value of \$0 also would not make logical sense.  If the last gift amount was \$0 then it shouldn't be counted as a donation. Again, if someone donated \$0.01 once, both `Previous.Gift` would be greater than 0 with a value of \$0.01. However, after the steps above, there weren't any remaining observations with `Previous.Gift` = \$0:
```{r}
length(nvo_cur_data[nvo_cur_data$Previous.Gift == 0, ]$Previous.Gift)
```

Final data set for analysis (including row and column counts) after all data preparation steps:
```{r}
nvo_cur_data
```

For model training and evaluation, a 80-20 train-test split on the data was leveraged:
```{r}
nvo_trn_idx  = sample(nrow(nvo_cur_data), size = trunc(0.80 * nrow(nvo_cur_data)))
nvo_trn_data = nvo_cur_data[nvo_trn_idx, ]
nvo_tst_data = nvo_cur_data[-nvo_trn_idx, ]
```

##### Model Selection Process

```{r}
# functions to be used throughout the document

# calculates LOOCV RMSE for a Model
get_loocv_rmse = function(model) {
  sqrt(mean((resid(model) / (1 - hatvalues(model))) ^ 2))
}

# calculates Adjusted R Squared Value for a Model
get_adj_r2 = function(model) {
  summary(model)$adj.r.squared
}


# finds observations with leverage greater than 2 times the average leverage
get_high_leverage = function(model){
  hatvalues(model) > 2 * mean(hatvalues(model))
}

# finds observations with a standardized residual greater than a magnitude of 2
get_outliers = function(model){
  abs(rstandard(model)) > 2
}

# finds observations with a cooks.distance greater than 4 / number of observations
get_high_influence = function(model){
  cooks.distance(model) > 4 / length(cooks.distance(model))
}

# uses Breusch-Pagan Test to validate Constant Variance assumption
get_bp_p_value = function(model) {
  unname(bptest(model)$p.value)
}

# uses Shapiro-Wilkes Test to validate Normality Assumption
get_sw_p_value = function(model) {
	if (length(resid(model)) > 5000) {
		p_val = shapiro.test(sample(resid(model),5000))$p.value
	} else {
		p_val = shapiro.test(resid(model))$p.value
	}
	unname(p_val)
}

# returns fitted versus residuals and Q-Q plots
diagnostic_plots = function(model, pcol = "grey", lcol = "dodgerblue") {

	par(mfrow=c(1,2))
	# A fitted versus residuals plot
	plot(fitted(model), resid(model), col = pcol, pch = 20,
		xlab = "Fitted", ylab = "Residuals", main = "Fitted vs Residuals Plot")
	abline(h = 0, col = lcol, lwd = 2)
	# A Normal Q-Q plot
	qqnorm(resid(model), main = "Normal Q-Q Plot", col = pcol)
	qqline(resid(model), col = lcol, lwd = 2)

}

# returns p-values of Breusch-Pagan and Shapiro-Wilkes tests 
get_model_diagnostics = function(model, pcol = "grey", lcol = "dodgerblue", plot_it = FALSE) {
	if (plot_it == TRUE) {
		diagnostic_plots(model, pcol, lcol)
	}
	res = c("BP Test P-Value" = get_bp_p_value(model),"SW Test P-Value" = get_sw_p_value(model)) 
	res
}

# returns LOOCV_RMSE and Adj. R-Squared values
evaluate_model = function(model) {
	res = c("LOOCV_RMSE" = get_loocv_rmse(model),"Adj. R-squared" = get_adj_r2(model)) 
	res
}
```

##### Model Selection Process

###### Progressing through different model iterations
```{r}
# simple additive model that includes all variables as predictors
add_cur_model = lm(Current.Gift ~ ., data = nvo_cur_data)

# includes all two-way interactions with each variable as a predictor
two_way_cur_model = lm(Current.Gift ~ . ^ 2, data = nvo_cur_data)

# maybe include histograms here??

# model including a mix of three way interactions and log transformations
cur_mod = lm(log(Current.Gift) ~ (log(Previous.Gift) + log(Largest.Gift) + log(Smallest.Gift)) ^ 3 + (Age + Own.Home. + Num.Children + Total.Wealth + Sex + Number.of.Gifts + Time.Between.Gifts + Other.Gifts) ^ 3, data = nvo_cur_data)

# model produced from backwards selection on above model (cur_mod)
#cur_mod_back_aic = step(cur_mod, direction = "backward", trace = 0)
```

###### Removing unusual observations
```{r}
# remove outliers
new_cur_data = nvo_cur_data[-get_outliers(cur_mod), ]
new_cur_mod = lm(log(Current.Gift) ~ (log(Previous.Gift) + log(Largest.Gift) + log(Smallest.Gift)) ^ 3 + (Age + Own.Home. + Num.Children + Total.Wealth + Sex + Number.of.Gifts + Time.Between.Gifts + Other.Gifts) ^ 3, data = new_cur_data)

# removed high leverage
new_cur_data = nvo_cur_data[-get_high_leverage(cur_mod), ]
new_cur_mod = lm(log(Current.Gift) ~ (log(Previous.Gift) + log(Largest.Gift) + log(Smallest.Gift)) ^ 3 + (Age + Own.Home. + Num.Children + Total.Wealth + Sex + Number.of.Gifts + Time.Between.Gifts + Other.Gifts) ^ 3, data = new_cur_data)

# removed high influence
new_cur_data = nvo_cur_data[-get_high_influence(cur_mod), ]
new_cur_mod = lm(log(Current.Gift) ~ (log(Previous.Gift) + log(Largest.Gift) + log(Smallest.Gift)) ^ 3 + (Age + Own.Home. + Num.Children + Total.Wealth + Sex + Number.of.Gifts + Time.Between.Gifts + Other.Gifts) ^ 3, data = new_cur_data)

```


### Results

### Appendix

#### Group Members

The following students contributed to this group project:

- Jake Goodman (NetID: jakeg5)
- Michael McClanahan (NetID: mjm31)


### References


***